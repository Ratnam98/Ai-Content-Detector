{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c657dded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\ratna\\anaconda\\lib\\site-packages (4.28.0.dev0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ratna\\anaconda\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\ratna\\anaconda\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ratna\\anaconda\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ratna\\anaconda\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ratna\\anaconda\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ratna\\anaconda\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ratna\\anaconda\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\ratna\\anaconda\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\ratna\\anaconda\\lib\\site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ratna\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ratna\\anaconda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ratna\\anaconda\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ratna\\anaconda\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ratna\\anaconda\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ratna\\anaconda\\lib\\site-packages (from requests->transformers) (2022.12.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ratna\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ratna\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ratna\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ratna\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ratna\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ratna\\anaconda\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd08accd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import GPTNeoForCausalLM, AutoTokenizer\n",
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n",
    "import torch\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1771a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10ce4682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large-openai-detector\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-openai-detector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "09df2220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the text: with his professional football career on the rise , davis had n't wanted to be shackled with the responsibilities of a baby \n"
     ]
    }
   ],
   "source": [
    "text=str(input(\"\"\"Enter the text: \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7455304f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"with his professional football career on the rise , davis had n't wanted to be shackled with the responsibilities of a baby \""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8b56774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=tokenizer(text,padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5d1a0fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sentences(text):\n",
    "    clean_text = text.replace('\\n', ' ')\n",
    "    return re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', clean_text)\n",
    "\n",
    "\n",
    "def chunks_of_2000(text, chunk_size = 2000):\n",
    "    sentences = text_to_sentences(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk + sentence) <= chunk_size:\n",
    "            if len(current_chunk)!=0:\n",
    "                current_chunk += \" \"+sentence\n",
    "            else:\n",
    "                current_chunk += sentence\n",
    "        else:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = sentence\n",
    "    chunks.append(current_chunk)\n",
    "    return chunks\n",
    "    \n",
    "def predict(query):\n",
    "    tokens = tokenizer.encode(query)\n",
    "    all_tokens = len(tokens)\n",
    "    tokens = tokens[:tokenizer.model_max_length - 2]\n",
    "    used_tokens = len(tokens)\n",
    "    tokens = torch.tensor([tokenizer.bos_token_id] + tokens + [tokenizer.eos_token_id]).unsqueeze(0)\n",
    "    mask = torch.ones_like(tokens)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      logits = model(**inputs).logits\n",
    "    \n",
    "      probs=logits.softmax(dim=-1)\n",
    "        \n",
    "\n",
    "    Ai_generated, Human_Generated = probs.detach().cpu().flatten().numpy().tolist()\n",
    "    return Ai_generated\n",
    "\n",
    "def findRealProb(text):\n",
    "    chunksOfText = (chunks_of_2000(text))\n",
    "    results = []\n",
    "    for chunk in chunksOfText:\n",
    "        output = predict(chunk)\n",
    "        results.append([output, len(chunk)])\n",
    "    \n",
    "    ans = 0\n",
    "    cnt = 0\n",
    "    for prob, length in results:\n",
    "        cnt += length\n",
    "        ans = ans + prob*length\n",
    "    realProb = ans/cnt\n",
    "    return {\"Ai_Generated\": realProb, \"Human_Generated\": 1-realProb}, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "585bc589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Ai_Generated': 0.18462973833084106, 'Human_Generated': 0.8153702616691589},\n",
       " [[0.18462973833084106, 124]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findRealProb(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dbda490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ai_percentage(text):\n",
    "    inputs=tokenizer(text,padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        ai_probs = logits.softmax(dim=-1)\n",
    "    ai_percentage = ai_probs[0][0] / len(ai_probs)\n",
    "    return ai_percentage * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "95a74334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(18.4630)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ai_percentage(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16866fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
